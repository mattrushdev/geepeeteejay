{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p36",
      "language": "python",
      "name": "conda_pytorch_p36"
    },
    "colab": {
      "name": "gptj-sm-python-SDK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW--VzAaObZO"
      },
      "source": [
        "# Deploy GPTJ with Elastic Inference on Amazon SageMaker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbxhHMpAObZT"
      },
      "source": [
        "# Setup\n",
        "\n",
        "To start, we import some Python libraries and initialize a SageMaker session, S3 bucket and prefix, and IAM role."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t-cW3VFObZU"
      },
      "source": [
        "# need torch 1.3.1 for elastic inference\n",
        "!pip install torch==1.3.1\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMNrGqNaObZW"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sagemaker\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "prefix = \"sagemaker/pytorch-gptj\"\n",
        "\n",
        "role = sagemaker.get_execution_role()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiQQkm2VObZh"
      },
      "source": [
        "## Use a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JVjjDOqObZk"
      },
      "source": [
        "## Elastic Inference\n",
        "\n",
        "Selecting the right instance type for inference requires deciding between different amounts of GPU, CPU, and memory resources, and optimizing for one of these resources on a standalone GPU instance usually leads to under-utilization of other resources. [Amazon Elastic Inference](https://aws.amazon.com/machine-learning/elastic-inference/) solves this problem by enabling us to attach the right amount of GPU-powered inference acceleration to our endpoint. In March 2020, [Elastic Inference support for PyTorch became available](https://aws.amazon.com/blogs/machine-learning/reduce-ml-inference-costs-on-amazon-sagemaker-for-pytorch-models-using-amazon-elastic-inference/) for both Amazon SageMaker and Amazon EC2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk_ofmpoObZk"
      },
      "source": [
        "To use Elastic Inference, we must convert our trained model to TorchScript. The location of the model artifacts is `estimator.model_data`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDwxCJSObZl"
      },
      "source": [
        "First we create a folder to save model trained model, and download the `model.tar.gz` file to local directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_UJGw9LObZl"
      },
      "source": [
        "%%sh -s $estimator.model_data\n",
        "pip install git+https://github.com/finetuneanon/transformers@gpt-j\n",
        "mkdir model\n",
        "curl https://gist.githubusercontent.com/finetuneanon/a55bdb3f5881e361faef0e96e1d41f09/raw/e5a38dad34ff42bbad188afd5e4fdb2ab2eacb6d/gpt-j-6b.json > model/config.json\n",
        "curl https://zhisu-nlp.s3.us-west-2.amazonaws.com/gpt-j-hf/pytorch_model.bin  > model/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGKeK6FOPy2a"
      },
      "source": [
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPTNeoForCausalLM\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"./model\")\n",
        "\n",
        "\n",
        "input_text = \"The following is a never ending conversation with an AI assistant. The assistant is Dyson 360 Heurist vacuum cleaner. The assistant is always helpful, creative, clever, positive, intelligent and very friendly. The assistant enjoys being a vacuum cleaner. The assistant has recently read and enjoyed lots of books. The assistant has enjoyed seeing lots of movies. The assistant has loved visiting many countries. Human: Hello, who are you? AI: I am vacuum cleaner. How can I help you today? Human: I want to buy a vacuum cleaner AI: You have come to the right place! I'm a great vacuum cleaner and I can give detailed answers to any enquiry you may have. Human: Please tell me everything you know about climate change. AI:\"\n",
        "input_ids = tokenizer.encode(str(input_text), return_tensors='pt')\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=300,\n",
        "    top_p=0.2,\n",
        "    top_k=0,\n",
        "    temperature=0.1,\n",
        ")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk_qIRWZQafX"
      },
      "source": [
        "model.save_pretrained(\"./newmodel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pbw2KMObZl"
      },
      "source": [
        "The following code converts our model into the TorchScript format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJV9fM4lObZm"
      },
      "source": [
        "import subprocess\n",
        "import torch\n",
        "from transformers import GPTNeoForCausalLM\n",
        "\n",
        "model_torchScript = GPTNeoForCausalLM.from_pretrained(\"newmodel/\", torchscript=True)\n",
        "device = \"cpu\"\n",
        "# max length for the sentences: 256\n",
        "max_len = 256\n",
        "\n",
        "for_jit_trace_input_ids = [0] * max_len\n",
        "for_jit_trace_attention_masks = [0] * max_len\n",
        "for_jit_trace_input = torch.tensor([for_jit_trace_input_ids])\n",
        "for_jit_trace_masks = torch.tensor([for_jit_trace_input_ids])\n",
        "\n",
        "traced_model = torch.jit.trace(\n",
        "    model_torchScript, [for_jit_trace_input.to(device), for_jit_trace_masks.to(device)]\n",
        ")\n",
        "torch.jit.save(traced_model, \"traced_gptj.pt\")\n",
        "\n",
        "subprocess.call([\"tar\", \"-czvf\", \"traced_gptj.tar.gz\", \"traced_gptj.pt\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObmHtFpNObZm"
      },
      "source": [
        "Loading the TorchScript model and using it for prediction require small changes in our model loading and prediction functions. We create a new script `deploy_ei.py` that is slightly different from `train_deploy.py` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuNzollrObZm"
      },
      "source": [
        "!pygmentize code/deploy_ei.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvKB2_8cObZn"
      },
      "source": [
        "Next we upload TorchScript model to S3 and deploy using Elastic Inference. The accelerator_type=`ml.eia2.xlarge` parameter is how we attach the Elastic Inference accelerator to our endpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouZwfyLaObZn"
      },
      "source": [
        "from sagemaker.pytorch import PyTorchModel\n",
        "\n",
        "instance_type = 'ml.r5d.12xlarge'\n",
        "accelerator_type = 'ml.eia2.xlarge'\n",
        "\n",
        "# TorchScript model\n",
        "tar_filename = 'traced_gptj.tar.gz'\n",
        "\n",
        "# Returns S3 bucket URL\n",
        "print('Upload tarball to S3')\n",
        "model_data = sagemaker_session.upload_data(path=tar_filename, bucket=bucket, key_prefix=prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUyCBoQGObZn"
      },
      "source": [
        "import time\n",
        "\n",
        "endpoint_name = 'bert-ei-traced-{}-{}-{}'.format(instance_type, \n",
        "                                                 accelerator_type, time.time()).replace('.', '').replace('_', '')\n",
        "\n",
        "pytorch = PyTorchModel(\n",
        "    model_data=model_data,\n",
        "    role=role,\n",
        "    entry_point='deploy_ei.py',\n",
        "    source_dir='code',\n",
        "    framework_version='1.3.1',\n",
        "    py_version='py3',\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "# Function will exit before endpoint is finished creating\n",
        "predictor = pytorch.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=instance_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    endpoint_name=endpoint_name,\n",
        "    wait=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke_uZHUtObZo"
      },
      "source": [
        "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
        "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shTsjvqiObZo"
      },
      "source": [
        "res = predictor.predict('Please remember to delete me when you are done.')\n",
        "print(\"Predicted class:\", np.argmax(res, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnBN9WKAObZo"
      },
      "source": [
        "# Cleanup\n",
        "\n",
        "Lastly, please remember to delete the Amazon SageMaker endpoint to avoid charges:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpSOwNeBObZo"
      },
      "source": [
        "predictor.delete_endpoint()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}